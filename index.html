
<!-- saved from url=(0029)http://web.stanford.edu/~ericyi/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=GBK">
<title>Li Yi</title>
<link rel="stylesheet" type="text/css" href="./Li_Yi_files/main.css">
</head>

<body>

<table>
<tbody><tr>
<td><img src="./Li_Yi_files/ericyi.jpg" width="200"></td>
<td>
<div style="font-size:24; font-weight:bold">Li Yi (弋力)</div>
<div>
Tenure-track Assistant Professor at Tsinghua University<br>
助理教授 博士生导师 清华大学交叉信息研究院（IIIS）<br>
</div>
<div>
<b>Email:</b> <tt>ericyi0124 at gmail dot com</tt><br>
</div>
<div>
<a href="https://scholar.google.com/citations?user=UyZL660AAAAJ&hl=en">[Google Scholar]</a><a href="https://twitter.com/ericyi0124">[Twitter]</a>
</div>
</td>
</tr>
</tbody></table>

<script type="text/javascript">
function hideshow(which){
if (!document.getElementById)
return
if (which.style.display=="block")
which.style.display="none"
else
which.style.display="block"
}
</script>



<h3>About</h3>
<div class="section">
<ul>
I am an Assistant Professor in the <a href="https://iiis.tsinghua.edu.cn/">Institute for Interdisciplinary Information Sciences (IIIS)</a> at Tsinghua University. I received my Ph.D. from Stanford University, advised by Professor <a href="http://geometry.stanford.edu/member/guibas/index.html">Leonidas J. Guibas</a>. And I spent a wonderful time at Google as a Research Scientist after graduation, working closely with Professor <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>. Prior to joining Stanford, I got my bachelor's degree in Electronic Engineering from <a href="http://www.tsinghua.edu.cn/">Tsinghua University</a>.</ul>
<ul>
My recent research interests focus on 3D perception, human-robot interaction and embodied AI, with the goal of equipping robotic agent with the ability of understanding and interacting with the 3D world.
</ul>
</div>
<br>

<h3>Recruiting</h3>
<div class="section">
<ul>
I am actively looking for motivated visiting students, interns, PhDs, and postdocs. Please feel free to email me if you are interested.
<li> For PhD applicants, please contact me at least half a year prior to your application.
<li> For visiting students or research interns, we have openings for long-term internship (six months or longer). Both undergraduate and graduate students are welcomed. Please email me with your CV and transcript to apply.
</ul>
</div>
<br>

<div class="section">
<h3>News</h3>
<ul>
<li> <b style="color: green; background-color: #ffff42">NEW</b> [2024/11] I am invited to be a speaker in <a href="https://dex-manipulation.github.io/corl2024/index.html">Learning Robot Fine and Dexterous Manipulation Workshop</a> at CoRL 2024 (<a href="https://www.youtube.com/watch?v=rtHeKZJEzSg&t=14867s">video recording</a>).
<li> <b style="color: green; background-color: #ffff42">NEW</b> [2024/11] Two papers accepted to 3DV 2025.
<li> <b style="color: green; background-color: #ffff42">NEW</b> [2024/09] One paper accepted to NeurIPS 2024.
<li> <b style="color: green; background-color: #ffff42">NEW</b> [2024/07] Three papers accepted to ECCV 2024 and one paper accepted to ACMMM 2024 as <font color="red">oral</font>.
<li> <b style="color: green; background-color: #ffff42">NEW</b> [2024/03] Four papers accepted to CVPR 2024.
<li> <b style="color: green; background-color: #ffff42">NEW</b> [2024/01] Two papers accepted to ICRA 2024 with one also accepted to Robotics and Automation Letters (RA-L).
<li> <b style="color: green; background-color: #ffff42">NEW</b> [2024/01] Two papers accepted to ICLR 2024 with one as <font color="red">spotlight</font>.
<li> [2023/12] Two papers accepted to AAAI 2024.
<li> [2023/07] Four papers accepted to ICCV 2023.
<li> [2023/06] I will serve as an Area Chair for <a href="https://cvpr.thecvf.com/Conferences/2024/">CVPR 2024</a>.
<li> [2023/04] One paper accepted to ICML 2023 and one paper accepted to SIGGRAPH 2023.
<li> [2023/03] I serve as an Area Chair for <a href="https://nips.cc/">NeurIPS 2023</a>.
<li> [2023/03] Seven papers accepted to CVPR 2023.
<li> [2023/01] Two papers accepted to ICLR 2023.
<li> [2023/01] Two papers accepted to AAAI 2023 as <font color="red">orals</font>.
<li> [2022/10] I serve as an Area Chair for <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>.
<li> [2022/09] One paper accepted to ECCV 2022 and one paper accepted to SIGGRAPH Asia 2022.
<li> [2022/03] Seven papers accepted to CVPR 2022.
<li> [2021/09] Two papers accepted to NeurIPS 2021 and one paper accepted to ICCV 2021.
<li> [2021/05] I serve as an Area Chair for <a href="http://cvpr2022.thecvf.com/">CVPR 2022</a>.
<li> [2021/05] I am organizing <a href="https://iccv21-seai.github.io/">The 1st Workshop on Simulation Technology for Embodied AI</a> at <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a>.
<li> [2021/03] Two papers accepted at CVPR 2021 (one oral included).
</ul>
</div>
<br>

<a name="projects"></a>
<h3>Recent Projects</h3>
<div class="mainsection">
<ul>

*: equivalent contribution, <span>†</span>: corresponding author
<br><br>


<table width="100%">


<!-- Thesis -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/thesis.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Deep Object-Centric 3D Perception</a></b><br><br><b>Li Yi</b><br><br>Ph.D. Thesis<br><br>
<a href="https://searchworks.stanford.edu/view/13333376">PDF</a>
</p><pre><p id="thesis" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- IHuman -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/ihuman.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Interactive Humanoid: Online Full-Body Motion Reaction Synthesis with Social Affordance Canonicalization and Forecasting</a></b><br><br>Yunze Liu, Changxi Chen, <b>Li Yi<span>†</span></b><br><br>3DV 2025<br><br>
<a href="https://arxiv.org/abs/2312.08983">PDF</a>
&nbsp<a href="https://yunzeliu.github.io/iHuman/">Project</a>
</p><pre><p id="ihuman" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- ImOV3D -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/imov3d.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">ImOV3D: Learning Open Vocabulary Point Clouds 3D Object Detection from Only 2D Images
</a></b><br><br>Timing Yang*, Yuanliang Ju*, <b>Li Yi<span>†</span></b><br><br>NeurIPS 2024<br><br>
<a href="https://arxiv.org/pdf/2410.24001v1">PDF</a>
&nbsp<a href="https://github.com/yangtiming/imov3d">Code</a>
</p><pre><p id="imov3d" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- ShapeLLM -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/shapellm.jpg" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">ShapeLLM: Universal 3D Object Understanding for Embodied Interaction</a></b><br><br>Zekun Qi, Runpei Dong, Shaochen Zhang, Haoran Geng, Chunrui Han, Zheng Ge, He Wang, <b>Li Yi<span>†</span></b>, Kaisheng Ma<span>†</span><br><br>ECCV 2024<br><br>
<a href="https://arxiv.org/abs/2402.17766">PDF</a>
&nbsp<a href="https://qizekun.github.io/shapellm/">Project</a>
&nbsp<a href="https://github.com/qizekun/ShapeLLM">Code</a>
</p><pre><p id="shapellm" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- QuasiSim -->
<tbody><tr>
<td width="20%" valign="top"><p><video autoplay="" muted="" loop="" playsinline="" width="250"><source src="./Li_Yi_files/quasisim.mp4" type="video/mp4"></video></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">QuasiSim: Parameterized Quasi-Physical Simulators for Dexterous Manipulations Transfer</a></b><br><br>Xueyi Liu, Kangbo Lyu, Jieqiong Zhang, Tao Du, <b>Li Yi<span>†</span></b><br><br>ECCV 2024<br><br>
<a href="https://arxiv.org/abs/2404.07988">PDF</a>
&nbsp<a href="https://meowuu7.github.io/QuasiSim/">Project</a>
&nbsp<a href="https://github.com/Meowuu7/QuasiSim">Code</a>
</p><pre><p id="quasisim" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- FreeMotion -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/freemotion.jpg" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">FreeMotion: MoCap-Free Human Motion Synthesis with Multimodal Large Language Models</a></b><br><br>Zhikai Zhang, Yitang Li, Haofeng Huang, Mingxian Lin, <b>Li Yi<span>†</span></b><br><br>ECCV 2024<br><br>
<a href="https://arxiv.org/abs/2406.10740">PDF</a>
&nbsp<a href="https://zzk273.github.io/freemotion.github.io/">Project</a>
</p><pre><p id="freemotion" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- PhysReact -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/physreact.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">PhysReaction: Physically Plausible Real-Time Humanoid Reaction Synthesis via Forward Dynamics Guided 4D Imitation
</a></b><br><br>Yunze Liu, Changxi Chen, Chenjing Ding, <b>Li Yi<span>†</span></b><br><br>ACMMM 2024 (<font color="red">Oral, top 3.9%</font>)<br><br>
<a href="https://arxiv.org/abs/2404.01081">PDF</a>
&nbsp<a href="https://yunzeliu.github.io/PhysReaction/">Project</a>
</p><pre><p id="physreact" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- GenH2R -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/genh2r.gif" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">GenH2R: Learning Generalizable Human-to-Robot Handover via Scalable Simulation, Demonstration, and Imitation</a></b><br><br>Zifan Wang*, Junyu Chen*, Ziqing Chen, Pengwei Xie, Rui Chen, <b>Li Yi<span>†</span></b><br><br> CVPR 2024<br><br>
<a href="https://arxiv.org/abs/2401.00929">PDF</a>
&nbsp<a href="https://genh2r.github.io/">Project</a>
</p><pre><p id="genh2r" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- TACO -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/taco.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">TACO: Benchmarking Generalizable Bimanual Tool-ACtion-Object Understanding</a></b><br><br>Yun Liu, Haolin Yang, Xu Si, Ling Liu, Zipeng Li, Yuxiang Zhang, Yebin Liu, <b>Li Yi<span>†</span></b><br><br> CVPR 2024<br><br>
<a href="https://arxiv.org/abs/2401.08399">PDF</a>
&nbsp<a href="https://taco2024.github.io/">Project</a>
</p><pre><p id="taco" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- GenN2N -->
<tbody><tr>
<td width="20%" valign="top"><p><video autoplay="" muted="" loop="" playsinline="" width="250"><source src="./Li_Yi_files/genn2n.mp4" type="video/mp4"></video></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">GenN2N: Generative NeRF2NeRF Translation</a></b><br><br>Xiangyue Liu, Han Xue, Kunming Luo, Ping Tan<span>†</span>, <b>Li Yi<span>†</span></b><br><br> CVPR 2024<br><br>
<a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Liu_GenN2N_Generative_NeRF2NeRF_Translation_CVPR_2024_paper.pdf">PDF</a>
&nbsp<a href="https://xiangyueliu.github.io/GenN2N/">Project</a>
&nbsp<a href="https://github.com/Lxiangyue/GenN2N">Code</a>
</p><pre><p id="genn2n" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- PhysDenoise -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/physdenoise.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Physics-aware Hand-object Interaction Denoising</a></b><br><br>Haowen Luo, Yunze Liu, <b>Li Yi<span>†</span></b><br><br> CVPR 2024<br><br>
<a href="https://arxiv.org/abs/2405.11481">PDF</a>
</p><pre><p id="physdenoise" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- DreamLLM -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/dreamllm.png" height="150" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">DreamLLM: Synergistic Multimodal Comprehension and Creation</a></b><br><br>Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, Xiangwen Kong, Xiangyu Zhang, Kaisheng Ma<span>†</span>, <b>Li Yi<span>†</span></b><br><br> ICLR 2024 (<font color="red">Spotlight, top 5%</font>)<br><br>
<a href="https://arxiv.org/abs/2309.11499">PDF</a>
&nbsp<a href="https://dreamllm.github.io/">Project</a>
&nbsp<a href="https://github.com/RunpeiDong/DreamLLM">Code</a>
</p><pre><p id="dreamllm" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- GeneOH -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/geneoh.gif" height="150" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">GeneOH Diffusion: Towards Generalizable Hand-Object Interaction Denoising via Denoising Diffusion</a></b><br><br>Xueyi Liu, <b>Li Yi<span>†</span></b><br><br> ICLR 2024<br><br>
<a href="https://openreview.net/pdf?id=FvK2noilxT">PDF</a>
&nbsp<a href="https://meowuu7.github.io/GeneOH-Diffusion/">Project</a>
&nbsp<a href="https://openreview.net/forum?id=FvK2noilxT">OpenReview</a>
</p><pre><p id="geneoh" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- TEG-Track -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/teg_track.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Enhancing Generalizable 6D Pose Tracking of an In-Hand Object with Tactile Sensing</a></b><br><br>Yun Liu*, Xiaomeng Xu*, Weihang Chen, Haocheng Yuan, He Wang, Jing Xu, Rui Chen, <b>Li Yi<span>†</span></b><br><br> RA-L with a presentation at ICRA 2024<br><br>
<a href="https://arxiv.org/abs/2210.04026">PDF</a>
</p><pre><p id="teg-track" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- CrossVideo -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/crossvideo.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">CrossVideo: Self-supervised Cross-modal Contrastive Learning for Point Cloud Video Understanding</a></b><br><br>Yunze Liu, Changxi Chen, Zifan Wang, <b>Li Yi<span>†</span></b><br><br> ICRA 2024<br><br>
<a href="https://arxiv.org/abs/2401.09057">PDF</a>
</p><pre><p id="crossvideo" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- SCSF -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/scsf.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Semantic Complete Scene Forecasting from a 4D Dynamic Point Cloud Sequence</a></b><br><br>Zifan Wang*, Zhuorui Ye*, Haoran Wu*, Junyu Chen, <b>Li Yi<span>†</span></b><br><br> AAAI 2024<br><br>
<a href="https://arxiv.org/abs/2312.08054">PDF</a>
&nbsp<a href="https://scsfnet.github.io/">Project</a>
</p><pre><p id="scsf" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>



<!-- MoRecon -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/morecon.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Full-Body Motion Reconstruction with Sparse Sensing from Graph Perspective</a></b><br><br>Feiyu Yao, Zongkai Wu<span>†</span>, <b>Li Yi<span>†</span></b><br><br> AAAI 2024<br><br>
<a href="https://arxiv.org/abs/2401.11783">PDF</a>
</p><pre><p id="morecon" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- NSM4D -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/nsm4d.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">NSM4D: Neural Scene Model Based Online 4D Point Cloud Sequence Understanding</a></b><br><br>Yuhao Dong*, Zhuoyang Zhang*, Yunze Liu, <b>Li Yi<span>†</span></b><br><br> arXiv:2310.08326 [cs.CV], Oct 2023<br><br>
<a href="https://arxiv.org/abs/2310.08326">PDF</a>
</p><pre><p id="nsm4d" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- TransTouch -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/transtouch.gif" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">TransTouch: Learning Transparent Objects Depth Sensing Through Sparse Touches</a></b><br><br>Liuyu Bian, Pengyang Shi, Weihang Chen, Jing Xu, <b>Li Yi<span>†</span></b>, Rui Chen<span>†</span><br><br> IROS 2023<br><br>
<a href="https://arxiv.org/pdf/2309.09427">PDF</a>
&nbsp<a href="https://youtu.be/WbkK1TZfy3M">Video</a>
&nbsp<a href="https://github.com/ritsu-a/transtouch">Code</a>
</p><pre><p id="transtouch" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- LeaF -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/leaf.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">LeaF: Learning Frames for 4D Point Cloud Sequence Understanding</a></b><br><br>Yunze Liu, Junyu Chen, Zekai Zhang, <b>Li Yi<span>†</span></b><br><br> ICCV 2023<br><br>
<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liu_LeaF_Learning_Frames_for_4D_Point_Cloud_Sequence_Understanding_ICCV_2023_paper.pdf">PDF</a>
</p><pre><p id="leaf" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- ArtGen -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/artgen.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation</a></b><br><br>Xueyi Liu, Bin Wang, He Wang, <b>Li Yi<span>†</span></b><br><br> ICCV 2023<br><br>
<a href="https://arxiv.org/abs/2308.10898">PDF</a>
&nbsp<a href="https://meowuu7.github.io/few-arti-obj-gen/">Project</a>
&nbsp<a href="https://github.com/Meowuu7/few-arti-gen">Code</a>
</p><pre><p id="artgen" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- UniDexGraspPlus -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/unidexgrasp++.jpg" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">UniDexGrasp++: Improving Dexterous Grasping Policy Learning via Geometry-aware Curriculum and Iterative Generalist-Specialist Learning</a></b><br><br>Weikang Wan*, Haoran Geng*, Yun Liu, Zikang Shan, Yaodong Yang, <b>Li Yi</b>, He Wang<br><br> ICCV 2023<br><br>
<a href="https://arxiv.org/abs/2304.00464">PDF</a>
&nbsp<a href="https://pku-epic.github.io/UniDexGrasp++/">Project</a>
&nbsp<a href="https://github.com/PKU-EPIC/UniDexGrasp2">Code</a>
</p><pre><p id="unidexgraspplus" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- 3DImplicitTransporter -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/3dimtransporter.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">3D Implicit Transporter for Temporally Consistent Keypoint Discovery</a></b><br><br>Chengliang Zhong, Yuhang Zheng, Yupeng Zheng, Hao Zhao, <b>Li Yi</b>, Xiaodong Mu, Ling Wang, Pengfei Li, Guyue Zhou, Chao Yang, Xinliang Zhang, Jian Zhao<br><br> ICCV 2023<br><br>
<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhong_3D_Implicit_Transporter_for_Temporally_Consistent_Keypoint_Discovery_ICCV_2023_paper.pdf">PDF</a>
&nbsp<a href="https://github.com/zhongcl-thu/3D-Implicit-Transporter">Code</a>
</p><pre><p id="3dimtransporter" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- ArrangementNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/arrangementnet.jpg" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">ArrangementNet: Learning Scene Arrangements for Vectorized Indoor Scene Modeling</a></b><br><br>Jingwei Huang, Shanshan Zhang, Bo Duan, Yanfeng Zhang, Xiaoyang Guo, Mingwei Sun, <b>Li Yi</b><br><br> SIGGRAPH 2023 (Journal Track)<br><br>
</p><pre><p id="arrangementnet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>


<!-- ReCon -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/recon.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining</a></b><br><br>Zekun Qi*, Runpei Dong*, Guofan Fan, Zheng Ge, Xiangyu Zhang, Kaisheng Ma<span>†</span>, <b>Li Yi<span>†</span></b><br><br> ICML 2023<br><br>
<a href="https://arxiv.org/pdf/2302.02318">PDF</a>
&nbsp<a href="https://github.com/qizekun/ReCon">Code</a>
</p><pre><p id="recon" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- C2P -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/c2p.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Complete-to-Partial 4D Distillation for Self-Supervised Point Cloud Sequence Representation Learning</a></b><br><br>Zhuoyang Zhang*, Yuhao Dong*, Yunze Liu, <b>Li Yi<span>†</span></b><br><br> CVPR 2023<br><br>
<a href="https://arxiv.org/abs/2212.05330">PDF</a>
&nbsp<a href="https://dongyh20.github.io/c2p.github.io/">Project</a>
&nbsp<a href="https://github.com/dongyh20/C2P">Code</a>
</p><pre><p id="c2p" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- CAMS -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/cams.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation Synthesis</a></b><br><br>Juntian Zheng*, Lixing Fang*, Qingyuan Zheng*, Yun Liu, <b>Li Yi<span>†</span></b><br><br> CVPR 2023<br><br>
<a href="https://arxiv.org/abs/2303.15469">PDF</a>
&nbsp<a href="https://cams-hoi.github.io/">Project</a>
&nbsp<a href="https://github.com/cams-hoi/CAMS">Code</a>
</p><pre><p id="cams" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- JacobiNeRF -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/jacobinerf.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">JacobiNeRF: NeRF Shaping with Mutual Information Gradients</a></b><br><br>Xiaomeng Xu, Yanchao Yang, Kaichun Mo, Boxiao Pan, <b>Li Yi</b>, Leonidas J. Guibas<br><br> CVPR 2023<br><br>
<a href="https://arxiv.org/abs/2304.00341">PDF</a>
&nbsp<a href="https://github.com/xxm19/jacobinerf">Code</a>
</p><pre><p id="jacobinerf" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- GAPartNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/gapartnet.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">GAPartNet: Learning Generalizable and Actionable Parts for Cross-Category Object Perception and Manipulation</a></b><br><br>Haoran Geng*, Helin Xu*, Chengyang Zhao*, Chao Xu, <b>Li Yi</b>, Siyuan Huang, He Wang<br><br> CVPR 2023<br><br>
<a href="https://arxiv.org/abs/2211.05272">PDF</a>
&nbsp<a href="https://pku-epic.github.io/GAPartNet/">Project</a>
</p><pre><p id="gapartnet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- UniDexGrasp -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/unidexgrasp.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy</a></b><br><br>Yinzhen Xu*, Weikang Wan*, Jialiang Zhang*, Haoran Liu*, Zikang Shan, Hao Shen, Ruicheng Wang, Haoran Geng, Yijia Weng, Jiayi Chen, Tengyu Liu, <b>Li Yi</b>, He Wang<br><br> CVPR 2023<br><br>
<a href="https://arxiv.org/abs/2303.00938">PDF</a>
&nbsp<a href="https://pku-epic.github.io/UniDexGrasp/">Project</a>
</p><pre><p id="unidexgrasp" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- SparseVIT -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/sparsevit.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer</a></b><br><br>Xuanyao Chen*, Zhijian Liu*, Haotian Tang, <b>Li Yi</b>, Hang Zhao, Song Han<br><br> CVPR 2023<br><br>
<a href="https://arxiv.org/abs/2303.17605">PDF</a>
</p><pre><p id="sparsevit" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- SWMoPred -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/swmopred.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Semi-Weakly Supervised Object Kinematic Motion Prediction</a></b><br><br>Gengxin Liu, Qian Sun, Haibin Huang, Chongyang Ma, Yulan Guo, <b>Li Yi</b>, Hui Huang, Ruizhen Hu<br><br> CVPR 2023<br><br>
<a href="https://arxiv.org/abs/2303.17774">PDF</a>
&nbsp<a href="https://github.com/GengxinLiu/SWMP">Code</a>
</p><pre><p id="swmopred" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- SelfArtPose -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/selfartpose.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance</a></b><br><br>Xueyi Liu, Ji Zhang, Ruizhen Hu, Haibin Huang, He Wang, <b>Li Yi<span>†</span></b><br><br> ICLR 2023<br><br>
<a href="https://arxiv.org/abs/2302.14268">PDF</a>
&nbsp<a href="https://equi-articulated-pose.github.io">Project</a>
&nbsp<a href="https://github.com/Meowuu7/equi-articulated-pose">Code</a>
</p><pre><p id="selfartpose" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- ACT -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/act.png" height="150" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?</a></b><br><br>Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, <b>Li Yi<span>†</span></b>, Kaisheng Ma<span>†</span><br><br> ICLR 2023<br><br>
<a href="https://arxiv.org/abs/2212.08320">PDF</a>
&nbsp<a href="https://github.com/RunpeiDong/ACT">Code</a>
</p><pre><p id="act" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- L3D -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/l3d.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Language-Assisted 3D Feature Learning for Semantic Scene Understanding</a></b><br><br>Junbo Zhang, Guofan Fan, Guanghan Wang, Zhengyuan Su, Kaisheng Ma<span>†</span>, <b>Li Yi<span>†</span></b><br><br> AAAI 2023 (<font color="red">Oral Presentation</font>)<br><br>
<a href="https://arxiv.org/abs/2211.14091">PDF</a>

</p><pre><p id="l3d" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- HOI-Track -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/trhoi.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild</a></b><br><br>Jiayi Chen*, Mi Yan*, Jiazhao Zhang, Yenzhen Xu, Xiaolong Li, Yijia Weng, <b>Li Yi</b>, Shuran Song, He Wang<br><br> AAAI 2023 (<font color="red">Oral Presentation</font>)<br><br>
<a href="http://arxiv.org/abs/2209.12009">PDF</a>
</p><pre><p id="hoi-track" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- MoRig -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/morig.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">MoRig: Motion-Aware Rigging of Character Meshes from Point Clouds</a></b><br><br>Zhan Xu, Yang Zhou, <b>Li Yi</b>, Evangelos Kalogerakis<br><br>SIGGRAPH Asia 2022<br><br>
<a href="https://zhan-xu.github.io/motion-rig/">Project</a>
</p><pre><p id="morig" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PPTr -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/pptr.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Point Primitive Transformer for Long-Term 4D Point Cloud Video Understanding</a></b><br><br>Hao Wen*, Yunze Liu*, Jingwei Huang, Bo Duan, <b>Li Yi<span>†</span></b><br><br>ECCV 2022<br><br>
<a href="https://arxiv.org/abs/2208.00281">PDF</a>
</p><pre><p id="pptr" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- HOI4D -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/hoi4d.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction</a></b><br><br>Yunze Liu*, Yun Liu*, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, <b>Li Yi<span>†</span></b><br><br>CVPR 2022<br><br>
<a href="https://arxiv.org/abs/2203.01577">PDF</a>
&nbsp<a href="https://hoi4d.github.io">Project</a>
</p><pre><p id="hoi4d" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- EON -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/eon.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Rotationally Equivariant 3D Object Detection</a></b><br><br>Hong-Xing Yu, Jiajun Wu, <b>Li Yi<span>†</span></b><br><br>CVPR 2022<br><br>
<a href="https://kovenyu.com/eon/static/yu2022eon.pdf">PDF</a>
&nbsp<a href="https://kovenyu.com/eon/">Project</a>
</p><pre><p id="eon" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- AutoGPart -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/autogpart.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">AutoGPart: Intermediate Supervision Search for Generalizable 3D Part Segmentation</a></b><br><br>Xueyi Liu, Xiaomeng Xu, Anyi Rao, Chuang Gan, <b>Li Yi<span>†</span></b><br><br>CVPR 2022<br><br>
<a href="https://arxiv.org/abs/2203.06558">PDF</a>
&nbsp<a href="https://autogpart.github.io">Project</a>
</p><pre><p id="autogpart" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- CodedVTR -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/codedvtr.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric Guidance</a></b><br><br>Tianchen Zhao, Niansong Zhang, Xuefei Ning, He Wang, <b>Li Yi<span>†</span></b>, Yu Wang<br><br>CVPR 2022<br><br>
<a href="https://arxiv.org/abs/2203.09887">PDF</a>
&nbsp<a href="https://a-suozhang.xyz/codedvtr.github.io">Project</a>
</p><pre><p id="codedvtr" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- MultiRobotMap -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/multirobotmap.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Multi-Robot Active Mapping via Neural Bipartite Graph Matching</a></b><br><br>Kai Ye*, Siyan Dong*, Qingnan Fan, He Wang, <b>Li Yi</b>, Fei Xia, Jue Wang, Baoquan Chen<br><br>CVPR 2022<br><br>
<a href="https://arxiv.org/abs/2203.16319">PDF</a>
</p><pre><p id="multirobotmap" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- APES -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/apes.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">APES: Articulated Part Extraction from Sprite Sheets</a></b><br><br>Zhan Xu, Matthew Fisher, Yang Zhou, Deepali Aneja, Rushikesh Dudhat, <b>Li Yi</b>, Evangelos Kalogerakis<br><br>CVPR 2022<br><br>
<a href="">PDF</a>
&nbsp<a href="https://zhan-xu.github.io/parts">Project</a>
</p><pre><p id="apes" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- FixingMal -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/fixingmal.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Fixing Malfunctional Objects With Learned Physical Simulation and Functional Prediction</a></b><br><br>Yining Hong, Kaichun Mo, <b>Li Yi</b>, Leonidas J. Guibas, Antonio Torralba, Joshua B. Tenenbaum, Chuang Gan<br><br>CVPR 2022<br><br>
<a href="http://fixing-malfunctional.csail.mit.edu/assets/00889.pdf">PDF</a>
&nbsp<a href="http://fixing-malfunctional.csail.mit.edu">Project</a>
</p><pre><p id="fixingmal" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PTR -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/ptr.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">PTR: A Benchmark for Part-based Conceptual, Relational, and Physical Reasoning</a></b><br><br>Yining Hong, <b>Li Yi</b>, Joshua B. Tenenbaum, Antonio Torralba, Chuang Gan<br><br>NeurIPS 2021<br><br>
<a href="https://arxiv.org/abs/2112.05136">PDF</a>
</p><pre><p id="ptr" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- ESSCOP -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/esscop.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Leveraging SE(3) Equivariance for Self-supervised Category-Level Object Pose Estimation from Point Clouds</a></b><br><br>Xiaolong Li, Yijia Weng, <b>Li Yi</b>, Leonidas J. Guibas, A. Lynn Abbott, Shuran Song, He Wang<br><br>NeurIPS 2021<br><br>
<a href="https://arxiv.org/abs/2111.00190">PDF</a>
</p><pre><p id="esscop" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- TupleInfoNCE -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/tupleinfonce.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Contrastive Multimodal Fusion with TupleInfoNCE</a></b><br><br>Yunze Liu, Qingnan Fan, Shanghang Zhang, Hao Dong, Thomas Funkhouser, <b>Li Yi<span>†</span></b><br><br>ICCV 2021<br><br>
<a href="https://arxiv.org/abs/2107.02575">PDF</a>
</p><pre><p id="tupleinfonce" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- P4Contrast -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/p4contrast.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">P4Contrast: Contrastive Learning with Pairs of Point-Pixel Pairs for RGB-D Scene Understanding</a></b><br><br>Yunze Liu<sup>*</sup>, <b>Li Yi</b><sup>*</sup>, Shanghang Zhang, Qingnan Fan, Thomas Funkhouser, Hao Dong (* equal contribution)<br><br>arXiv:2012.13089 [cs.CV], Dec 2020<br><br>
<a href="https://arxiv.org/abs/2012.13089">PDF</a>
</p><pre><p id="p4contrast" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- GenStruct -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/genstruct.gif" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Compositionally Generalizable 3D Structure Prediction</a></b><br><br>Songfang Han, Jiayuan Gu, Kaichun Mo, <b>Li Yi</b>, Siyu Hu, Xuejin Chen and Hao Su<br><br>arXiv:2012.02493 [cs.CV], Dec 2020<br><br>
<a href="https://arxiv.org/abs/2012.02493">PDF</a>
</p><pre><p id="genstruct" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- NeuralRouting -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/neuralrouting.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments</a></b><br><br>Siyan Dong<sup>*</sup>, Qingnan Fan<sup>*</sup>, He Wang, Ji Shi, <b>Li Yi</b>, Thomas Funkhouser, Baoquan Chen, Leonidas J. Guibas (* equal contribution)<br><br>CVPR 2021 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/2012.04746">PDF</a>
</p><pre><p id="neuralrouting" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- CompLabel -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/complete_and_label.png" height="90" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Complete & Label: A Domain Adaptation Approach to Semantic Segmentation of LiDAR Point Clouds</a></b><br><br><b>Li Yi</b>, Boqing Gong, Thomas Funkhouser<br><br>CVPR 2021<br><br>
<a href="https://arxiv.org/abs/2007.08488">PDF</a>
</p><pre><p id="complabel" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- RethinkPCGan -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/rethinkpcgan.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Rethinking Sampling in 3D Point Cloud Generative Adversarial Networks</a></b><br><br>He Wang<sup>*</sup>, Zetian Jiang<sup>*</sup>, <b>Li Yi</b>, Kaichun Mo, Hao Su, Leonidas J. Guibas (* equal contribution)<br><br>CVPR 2021 Workshop on Learning to Generate 3D Shapes and Scenes<br><br>
<a href="https://arxiv.org/abs/2006.07029">PDF</a>
</p><pre><p id="rethinkpcgan" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- CSDF -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/csdf.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Curriculum DeepSDF</a></b><br><br>Yueqi Duan<sup>*</sup>, Haidong Zhu<sup>*</sup>, He Wang, <b>Li Yi</b>, Ram Nevatia, Leonidas J. Guibas (* equal contribution)<br><br>ECCV 2020<br><br>
<a href="https://arxiv.org/abs/2003.08593">PDF</a>
</p><pre><p id="csdf" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- Sapien -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/sapien.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">SAPIEN: A SimulAted Part-based Interactive ENvironment</a></b><br><br>Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, <b>Li Yi</b>, Angel X.Chang, Leonidas J. Guibas, Hao Su<br><br>CVPR 2020 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/2003.08515">PDF</a>
&nbsp<a href="http://sapien.ucsd.edu/publication">Project</a>
&nbsp<a href="https://github.com/haosulab/SAPIEN-Release">Code</a>
&nbsp<a href="https://youtu.be/K2yOeJhJXzM">Demo</a>
</p><pre><p id="sapien" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- ArticulatedPose -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/artpose.jpg" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Category-Level Articulated Object Pose Estimation</a></b><br><br>Xiaolong Li<sup>*</sup>, He Wang<sup>*</sup>, <b>Li Yi</b>, Leonidas Guibas, A. Lynn Abbott, Shuran Song (* equal contribution)<br><br>CVPR 2020 Oral Presentation<br><br>
<a href="https://articulated-pose.github.io/paper.pdf">PDF</a>
&nbsp<a href="https://articulated-pose.github.io/">Project</a>
</p><pre><p id="artpose" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- StructureEdit -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/structedit.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">StructEdit: Learning Structural Shape Variations</a></b><br><br>Kaichun Mo<sup>*</sup>, Paul Guerrero<sup>*</sup>, <b>Li Yi</b>, Hao Su, Peter Wonka, Niloy Mitra, Leonidas J. Guibas (* equal contribution)<br><br>CVPR 2020<br><br>
<a href="https://arxiv.org/abs/1911.11098">PDF</a>
&nbsp<a href="https://cs.stanford.edu/~kaichun/structedit/">Project</a>
</p><pre><p id="structedit" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- AdaCoSeg -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/zhu_adacoseg_long.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">AdaCoSeg: Adaptive Shape Co-Segmentation with Group Consistency Loss</a></b><br><br>Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, <b>Li Yi</b>, Leonidas J. Guibas, Hao Zhang<br><br>CVPR 2020 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/1903.10297">PDF</a>
</p><pre><p id="adaseg" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- StructureNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/structurenet.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">StructureNet: Hierarchical Graph Networks for 3D Shape Generation</a></b><br><br>Kaichun Mo<sup>*</sup>, Paul Guerrero<sup>*</sup>, <b>Li Yi</b>, Hao Su, Peter Wonka, Niloy Mitra, Leonidas J. Guibas (* equal contribution)<br><br>SIGGRAPH Asia 2019<br><br>
<a href="https://arxiv.org/abs/1908.00575">PDF</a>
&nbsp<a href="https://cs.stanford.edu/~kaichun/structurenet/">Project</a>
</p><pre><p id="structurenet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- GSPN -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/gspn.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud</a></b><br><br><b>Li Yi</b>, Wang Zhao, He Wang, Minhyuk Sung, Leonidas Guibas<br><br>CVPR 2019<br><br>
<a href="https://arxiv.org/abs/1812.03320">PDF</a>
&nbsp<a href="https://github.com/ericyi/GSPN">Code</a>
</p><pre><p id="gspn" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- TextureNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/texturenet.png" height="80" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">TextureNet: Consistent Local Parametrizations for Learning from High-Resolution Signals on Meshes</a></b><br><br>Jingwei Huang, Haotian Zhang, <b>Li Yi</b>, Thomas Funkhouser, Matthias Niessner, Leonidas Guibas<br><br>CVPR 2019 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/1812.00020">PDF</a>
</p><pre><p id="texturenet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PrimitiveFitNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/primitivefit.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Supervised Fitting of Geometric Primitives to 3D Point Clouds </a></b><br><br>Lingxiao Li<sup>*</sup>, Minhyuk Sung<sup>*</sup>, Anastasia Dubrovina, <b>Li Yi</b>, Leonidas Guibas (* equal contribution)<br><br>CVPR 2019 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/1811.08988">PDF</a>
&nbsp<a href="https://github.com/csimstu2/SPFN">Code</a>
</p><pre><p id="primitivefitnet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PartNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/partnet.png" height="140" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding</a></b><br><br>Kaichun Mo, Shilin Zhu, Angel X.Chang, <b>Li Yi</b>, Subarna Tripathi, Leonidas J. Guibas and Hao Su<br><br>CVPR 2019<br><br>
<a href="https://arxiv.org/abs/1812.02713">PDF</a>
&nbsp<a href="https://cs.stanford.edu/~kaichun/partnet/">Project</a>
</p><pre><p id="partnet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- GeoNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/geonet.jpg" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">GeoNet: Deep Geodesic Networks for Point Cloud Analysis</a></b><br><br>Tong He, Haibin Huang, <b>Li Yi</b>, Yuqian Zhou, Qihao Wu, Jue Wang, Stefano Soatto<br><br>CVPR 2019 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/1901.00680">PDF</a>
</p><pre><p id="geonet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PartMobility -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/partmobility.jpg" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Deep Part Induction from Articulated Object Pairs</a></b><br><br><b>Li Yi</b>, Haibin Huang, Difan Liu, Evangelos Kalogerakis, Hao Su, Leonidas Guibas<br><br>SIGGRAPH Asia 2018<br><br>
<a href="https://arxiv.org/pdf/1809.07417.pdf">PDF</a>
&nbsp<a href="https://github.com/ericyi/articulated-part-induction">Code</a>
</p><pre><p id="partmobility" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PartState -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/partstate.jpg" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">
Beyond Holistic Object Recognition: Enriching Image Understanding with Part States</a></b><br><br>Cewu Lu, Hao Su, Yongyi Lu, <b>Li Yi</b>, Chikeung Tang, Leonidas Guibas<br><br>CVPR 2018<br><br>
<a href="https://arxiv.org/pdf/1612.07310.pdf">PDF</a>
</p><pre><p id="partstate" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PointNet++ -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/pointnet2.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a></b><br><br>Charles R. Qi, <b>Li Yi</b>, Hao Su, Leonidas J. Guibas<br><br>NIPS 2017<br><br>
<a href="https://arxiv.org/pdf/1706.02413">PDF</a>
&nbsp<a href="http://stanford.edu/~rqi/pointnet2/">Project</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- HierSeg -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/hierseg.jpeg" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Learning Hierarchical Shape Segmentation and Labeling from Online Repositories</a></b><br><br><b>Li Yi</b>, Leonidas J. Guibas, Aaron Hertzmann, Vladimir G. Kim, Hao Su, Ersin Yumer <br><br>SIGGRAPH 2017<br><br>
<a href="https://arxiv.org/pdf/1705.01661.pdf">PDF</a>
&nbsp<a href="http://cs.stanford.edu/~ericyi/project_page/hier_seg/index.html">Project</a>
</p><pre><p id="hierseg" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- SyncSpecCNN -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/syncspeccnn.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation</a></b><br><br><b>Li Yi</b>, Hao Su, Xingwen Guo, Leonidas J. Guibas <br><br>CVPR 2017 Spotlight Presentation<br><br>
<a href="https://arxiv.org/pdf/1612.00606v1.pdf">PDF</a>
&nbsp<a href="https://github.com/ericyi/SyncSpecCNN">Code</a>
</p><pre><p id="syncspeccnn" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>




<!-- PartAnnotation -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/partanno.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">A Scalable Active Framework for Region Annotation in 3D Shape Collections</a></b><br><br><b>Li Yi</b>, Vladimir G. Kim, Duygu Ceylan, I-Chao Shen, Mengyuan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, Leonidas J. Guibas <br><br>SIGGRAPH Asia 2016<br><br>
<a href="http://cs.stanford.edu/~ericyi/papers/part_annotation_16.pdf">PDF(23.2MB)</a>
&nbsp<a href="http://cs.stanford.edu/~ericyi/papers/part_annotation_16_small.pdf">PDF(6.8MB)</a>
&nbsp<a href="http://cs.stanford.edu/~ericyi/papers/part_annotation_16_supplemental.pdf">Supplemental</a>
&nbsp<a href="http://cs.stanford.edu/~ericyi/project_page/part_annotation/index.html">Project</a>
&nbsp<a href="http://cs.stanford.edu/~ericyi/project_page/part_annotation/bib.html">Bib</a>
</p><pre><p id="partanno" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>



<!-- ShapeNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/shapenet.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="http://arxiv.org/abs/1512.03012">ShapeNet: An Information-Rich 3D Model Repository</a></b><br><br> Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, <b>Li Yi</b>, and Fisher Yu <br><br>arXiv:1512.03012 [cs.GR], Dec 2015<br><br>
<a href="http://arxiv.org/abs/1512.03012">PDF</a>
</p><pre><p id="shapenet" style="font:18px; display: none">
    @article{chang2015shapenet,
    title={Shapenet: An information-rich 3d model repository},
    author={Chang, Angel X and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and others},
    journal={arXiv preprint arXiv:1512.03012},
    year={2015}
    }
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- Novel View Feature Synthesis -->
<tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/surrogate.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="http://geometry.stanford.edu//papers/swyg-3afsnvo-15/swyg-3afsnvo-15.pdf">3D-Assisted Image Feature Synthesis for Novel Views of an Object</a></b><br><br>Hao Su<sup>*</sup>, Fan Wang<sup>*</sup>, <b>Li Yi</b>, Leonidas Guibas <br><br>ICCV 2015 oral<br><br>
<a href="http://geometry.stanford.edu//papers/swyg-3afsnvo-15/swyg-3afsnvo-15.pdf">PDF</a>
</p><pre><p id="surrogate" style="font:18px; display: none">
    @inproceedings{su20153d,
    title={3D-assisted feature synthesis for novel views of an object},
    author={Su, Hao and Wang, Fan and Yi, Eric and Guibas, Leonidas J},
    booktitle={Proceedings of the IEEE International Conference on Computer Vision},
    pages={2677--2685},
    year={2015}
    }
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- Super Resolution -->
<tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/superresolution.png" height="120" alt=""></p></td>
<td width="80%" valign="top">
<p>
<b><a href="http://cs.stanford.edu/~ericyi/papers/super_resolution_13.pdf">Image Super-Resolution Via Analysis Sparse Prior</a></b><br><br> Qiang Ning, Kan Chen, <b>Li Yi</b>, Chuchu Fan, Yao Lu, Jiangtao Wen <br><br>IEEE Signal Processing Letter 2013<br><br>
<a href="http://cs.stanford.edu/~ericyi/papers/super_resolution_13.pdf">PDF</a>
</p><pre><p id="superresolution" style="font:18px; display: none">
    @article{ning2013image,
    title={Image Super-Resolution Via Analysis Sparse Prior.},
    author={Ning, Qiang and Chen, Kan and Yi, Li and Fan, Chuchu and Lu, Yao and Wen, Jiangtao},
    journal={IEEE Signal Process. Lett.},
    volume={20},
    number={4},
    pages={399--402},
    year={2013}
    }
</p><p></p></pre>
<p></p></td>
</tr>

</tbody></table>

</ul>
</div>
<br>

<hr>
<div id="footer" style="font-size:10">Li Yi <i>Last updated: Nov, 2024</i></div>

</body></html>
