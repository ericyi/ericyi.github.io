
<!-- saved from url=(0029)http://web.stanford.edu/~ericyi/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=GBK">
<title>Li Yi</title>
<link rel="stylesheet" type="text/css" href="./Li_Yi_files/main.css">
</head>

<body>

<table>
<tbody><tr>
<td><img src="./Li_Yi_files/ericyi.jpg" width="200"></td>
<td>
<div style="font-size:24; font-weight:bold">Li Yi (弋力)</div>
<div>
Tenure-track Assistant Professor at Tsinghua University<br>
助理教授 博士生导师 清华大学交叉信息研究院（IIIS）<br>
</div>
<div>
<b>Email:</b> <tt>ericyi0124 at gmail dot com</tt><br>
</div>
<div>
<a href="https://scholar.google.com/citations?user=UyZL660AAAAJ&hl=en">[Google Scholar]</a><a href="https://twitter.com/ericyi0124">[Twitter]</a>
</div>
</td>
</tr>
</tbody></table>

<script type="text/javascript">
function hideshow(which){
if (!document.getElementById)
return
if (which.style.display=="block")
which.style.display="none"
else
which.style.display="block"
}
</script>



<h3>About</h3>
<div class="section">
<ul>
I am an Assistant Professor in the <a href="https://iiis.tsinghua.edu.cn/">Institute for Interdisciplinary Information Sciences (IIIS)</a> at Tsinghua University. I received my Ph.D. from Stanford University, advised by Professor <a href="http://geometry.stanford.edu/member/guibas/index.html">Leonidas J. Guibas</a>. And I spent a wonderful time at Google as a Research Scientist after graduation, working closely with Professor <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>. Prior to joining Stanford, I got my bachelor degree in Electronic Engineering from <a href="http://www.tsinghua.edu.cn/">Tsinghua University</a>.</ul>
<ul>
My recent research interests focus on 3D perception, geometry processing and embodied AI, with the goal of equipping robotic agent with the ability of understanding and interacting with the 3D world. I am actively looking for motivated PostDoc/PhD/BS students in my lab. Please feel free to email me with your resume if you are interested.
</ul>
</div>
<br>

<div class="section">
<h3>News</h3>
<ul>
<li> <b style="color: green; background-color: #ffff42">NEW</b> [2023/03] Seven papers accepted to CVPR 2023.
<li> <b style="color: green; background-color: #ffff42">NEW</b> [2023/01] Two papers accepted to ICLR 2023.
<li> <b style="color: green; background-color: #ffff42">NEW</b> [2023/01] Two papers accepted to AAAI 2023 as <font color="red">oral presentation</font>.
<li> [2022/10] I serve as an Area Chair for <a href="https://cvpr2023.thecvf.com/">CVPR 2023</a>
<li> [2022/09] One paper accepted to ECCV 2022 and one paper accepted to SIGGRAPH Asia 2022.
<li> [2022/03] Seven papers accepted to CVPR 2022.
<li> [2021/09] Two papers accepted to NeurIPS 2021 and one paper accepted to ICCV 2021.
<li> [2021/05] I serve as an Area Chair for <a href="http://cvpr2022.thecvf.com/">CVPR 2022</a>.
<li> [2021/05] I am organizing <a href="https://iccv21-seai.github.io/">The 1st Workshop on Simulation Technology for Embodied AI</a> at <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a>.
<li> [2021/03] Two papers accepted at CVPR 2021 (one oral included).
</ul>
</div>
<br>

<a name="projects"></a>
<h3>Recent Projects</h3>
<div class="mainsection">
<ul>

*: equivalent contribution, <span>†</span>: corresponding author
<br><br>


<table width="100%">


<!-- Thesis -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/thesis.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Deep Object-Centric 3D Perception</a></b><br><br><b>Li Yi</b><br><br>Ph.D. Thesis<br><br>
<a href="https://searchworks.stanford.edu/view/13333376">PDF</a>
</p><pre><p id="thesis" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- C2P -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/c2p.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Complete-to-Partial 4D Distillation for Self-Supervised Point Cloud Sequence Representation Learning</a></b><br><br>Zhuoyang Zhang*, Yuhao Dong*, Yunze Liu, <b>Li Yi<span>†</span></b><br><br> CVPR 2023<br><br>
<a href="https://arxiv.org/abs/2212.05330">PDF</a>
</p><pre><p id="c2p" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- CAMS -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/cams.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation Synthesis</a></b><br><br>Juntian Zheng*, Lixing Fang*, Qingyuan Zheng*, Yun Liu, <b>Li Yi<span>†</span></b><br><br> CVPR 2023<br><br>
</p><pre><p id="cams" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- JacobiNeRF -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/jacobinerf.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">JacobiNeRF: NeRF Shaping with Mutual Information Gradients</a></b><br><br>Xiaomeng Xu, Yanchao Yang, Kaichun Mo, Boxiao Pan, <b>Li Yi</b>, Leonidas J. Guibas<br><br> CVPR 2023<br><br>
</p><pre><p id="jacobinerf" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- GAPartNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/gapartnet.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">GAPartNet: Learning Generalizable and Actionable Parts for Cross-Category Object Perception and Manipulation</a></b><br><br>Haoran Geng*, Helin Xu*, Chengyang Zhao*, Chao Xu, <b>Li Yi</b>, Siyuan Huang, He Wang<br><br> CVPR 2023<br><br>
<a href="https://arxiv.org/abs/2211.05272">PDF</a>
</p><pre><p id="gapartnet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- UniDexGrasp -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/unidexgrasp.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">UniDexGrasp: Universal Robotic Dexterous Grasping via Learning Diverse Proposal Generation and Goal-Conditioned Policy</a></b><br><br>Yinzhen Xu*, Weikang Wan*, Jialiang Zhang*, Haoran Liu*, Zikang Shan, Hao Shen, Ruicheng Wang, Haoran Geng, Yijia Weng, Jiayi Chen, Tengyu Liu, <b>Li Yi</b>, He Wang<br><br> CVPR 2023<br><br>
</p><pre><p id="unidexgrasp" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- SparseVIT -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/sparsevit.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">SparseViT: Revisiting Activation Sparsity for Efficient High-Resolution Vision Transformer</a></b><br><br>Xuanyao Chen*, Zhijian Liu*, Haotian Tang, <b>Li Yi</b>, Hang Zhao, Song Han<br><br> CVPR 2023<br><br>
</p><pre><p id="sparsevit" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- SWMoPred -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/swmopred.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Semi-Weakly Supervised Object Kinematic Motion Prediction</a></b><br><br>Gengxin Liu, Qian Sun, Haibin Huang, Chongyang Ma, Yulan Guo, <b>Li Yi</b>, Hui Huang, Ruizhen Hu<br><br> CVPR 2023<br><br>
</p><pre><p id="swmopred" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- SelfArtPose -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/selfartpose.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Self-Supervised Category-Level Articulated Object Pose Estimation with Part-Level SE(3) Equivariance</a></b><br><br>Xueyi Liu, Ji Zhang, Ruizhen Hu, Haibin Huang, He Wang, <b>Li Yi<span>†</span></b><br><br> ICLR 2023<br><br>
</p><pre><p id="selfartpose" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- ACT -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/act.png" height="150" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?</a></b><br><br>Runpei Dong, Zekun Qi, Linfeng Zhang, Junbo Zhang, Jianjian Sun, Zheng Ge, <b>Li Yi<span>†</span></b>, Kaisheng Ma<span>†</span><br><br> ICLR 2023<br><br>
<a href="https://arxiv.org/abs/2212.08320">PDF</a>
&nbsp<a href="https://github.com/RunpeiDong/ACT">Code</a>
</p><pre><p id="act" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- L3D -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/l3d.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild</a></b><br><br>Junbo Zhang, Guofan Fan, Guanghan Wang, Zhengyuan Su, Kaisheng Ma<span>†</span>, <b>Li Yi<span>†</span></b><br><br> AAAI 2023 (<font color="red">Oral Presentation</font>)<br><br>
<a href="https://arxiv.org/abs/2211.14091">PDF</a>
</p><pre><p id="l3d" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- HOI-Track -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/trhoi.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild</a></b><br><br>Jiayi Chen*, Mi Yan*, Jiazhao Zhang, Yenzhen Xu, Xiaolong Li, Yijia Weng, <b>Li Yi</b>, Shuran Song, He Wang<br><br> AAAI 2023 (<font color="red">Oral Presentation</font>)<br><br>
<a href="http://arxiv.org/abs/2209.12009">PDF</a>
</p><pre><p id="hoi-track" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- TEG-Track -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/teg_track.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Enhancing Generalizable 6D Pose Tracking of an In-Hand Object with Tactile Sensing</a></b><br><br>Xiaomeng Xu*, Yun Liu*, Weihang Chen, Haocheng Yuan, He Wang, Jing Xu, Rui Chen, <b>Li Yi<span>†</span></b><br><br> arXiv:2210.04026 [cs.CV], Oct 2022<br><br>
<a href="https://arxiv.org/abs/2210.04026">PDF</a>
</p><pre><p id="teg-track" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- MoRig -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/morig.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">MoRig: Motion-Aware Rigging of Character Meshes from Point Clouds</a></b><br><br>Zhan Xu, Yang Zhou, <b>Li Yi</b>, Evangelos Kalogerakis<br><br>SIGGRAPH Asia 2022<br><br>
<a href="https://zhan-xu.github.io/motion-rig/">Project</a>
</p><pre><p id="morig" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PPTr -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/pptr.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Point Primitive Transformer for Long-Term 4D Point Cloud Video Understanding</a></b><br><br>Hao Wen*, Yunze Liu*, Jingwei Huang, Bo Duan, <b>Li Yi<span>†</span></b><br><br>ECCV 2022<br><br>
<a href="https://arxiv.org/abs/2208.00281">PDF</a>
</p><pre><p id="pptr" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- HOI4D -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/hoi4d.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction</a></b><br><br>Yunze Liu*, Yun Liu*, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, <b>Li Yi<span>†</span></b><br><br>CVPR 2022<br><br>
<a href="https://arxiv.org/abs/2203.01577">PDF</a>
&nbsp<a href="https://hoi4d.github.io">Project</a>
</p><pre><p id="hoi4d" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- EON -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/eon.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Rotationally Equivariant 3D Object Detection</a></b><br><br>Hong-Xing Yu, Jiajun Wu, <b>Li Yi<span>†</span></b><br><br>CVPR 2022<br><br>
<a href="https://kovenyu.com/eon/static/yu2022eon.pdf">PDF</a>
&nbsp<a href="https://kovenyu.com/eon/">Project</a>
</p><pre><p id="eon" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- AutoGPart -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/autogpart.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">AutoGPart: Intermediate Supervision Search for Generalizable 3D Part Segmentation</a></b><br><br>Xueyi Liu, Xiaomeng Xu, Anyi Rao, Chuang Gan, <b>Li Yi<span>†</span></b><br><br>CVPR 2022<br><br>
<a href="https://arxiv.org/abs/2203.06558">PDF</a>
&nbsp<a href="https://autogpart.github.io">Project</a>
</p><pre><p id="autogpart" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- CodedVTR -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/codedvtr.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric Guidance</a></b><br><br>Tianchen Zhao, Niansong Zhang, Xuefei Ning, He Wang, <b>Li Yi<span>†</span></b>, Yu Wang<br><br>CVPR 2022<br><br>
<a href="https://arxiv.org/abs/2203.09887">PDF</a>
&nbsp<a href="https://a-suozhang.xyz/codedvtr.github.io">Project</a>
</p><pre><p id="codedvtr" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- MultiRobotMap -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/multirobotmap.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Multi-Robot Active Mapping via Neural Bipartite Graph Matching</a></b><br><br>Kai Ye*, Siyan Dong*, Qingnan Fan, He Wang, <b>Li Yi</b>, Fei Xia, Jue Wang, Baoquan Chen<br><br>CVPR 2022<br><br>
<a href="https://arxiv.org/abs/2203.16319">PDF</a>
</p><pre><p id="multirobotmap" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- APES -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/apes.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">APES: Articulated Part Extraction from Sprite Sheets</a></b><br><br>Zhan Xu, Matthew Fisher, Yang Zhou, Deepali Aneja, Rushikesh Dudhat, <b>Li Yi</b>, Evangelos Kalogerakis<br><br>CVPR 2022<br><br>
<a href="">PDF</a>
&nbsp<a href="https://zhan-xu.github.io/parts">Project</a>
</p><pre><p id="apes" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- FixingMal -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/fixingmal.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Fixing Malfunctional Objects With Learned Physical Simulation and Functional Prediction</a></b><br><br>Yining Hong, Kaichun Mo, <b>Li Yi</b>, Leonidas J. Guibas, Antonio Torralba, Joshua B. Tenenbaum, Chuang Gan<br><br>CVPR 2022<br><br>
<a href="http://fixing-malfunctional.csail.mit.edu/assets/00889.pdf">PDF</a>
&nbsp<a href="http://fixing-malfunctional.csail.mit.edu">Project</a>
</p><pre><p id="fixingmal" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PTR -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/ptr.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">PTR: A Benchmark for Part-based Conceptual, Relational, and Physical Reasoning</a></b><br><br>Yining Hong, <b>Li Yi</b>, Joshua B. Tenenbaum, Antonio Torralba, Chuang Gan<br><br>NeurIPS 2021<br><br>
<a href="https://arxiv.org/abs/2112.05136">PDF</a>
</p><pre><p id="ptr" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- ESSCOP -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/esscop.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Leveraging SE(3) Equivariance for Self-supervised Category-Level Object Pose Estimation from Point Clouds</a></b><br><br>Xiaolong Li, Yijia Weng, <b>Li Yi</b>, Leonidas J. Guibas, A. Lynn Abbott, Shuran Song, He Wang<br><br>NeurIPS 2021<br><br>
<a href="https://arxiv.org/abs/2111.00190">PDF</a>
</p><pre><p id="esscop" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- TupleInfoNCE -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/tupleinfonce.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Contrastive Multimodal Fusion with TupleInfoNCE</a></b><br><br>Yunze Liu, Qingnan Fan, Shanghang Zhang, Hao Dong, Thomas Funkhouser, <b>Li Yi<span>†</span></b><br><br>ICCV 2021<br><br>
<a href="https://arxiv.org/abs/2107.02575">PDF</a>
</p><pre><p id="tupleinfonce" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- P4Contrast -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/p4contrast.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">P4Contrast: Contrastive Learning with Pairs of Point-Pixel Pairs for RGB-D Scene Understanding</a></b><br><br>Yunze Liu<sup>*</sup>, <b>Li Yi</b><sup>*</sup>, Shanghang Zhang, Qingnan Fan, Thomas Funkhouser, Hao Dong (* equal contribution)<br><br>arXiv:2012.13089 [cs.CV], Dec 2020<br><br>
<a href="https://arxiv.org/abs/2012.13089">PDF</a>
</p><pre><p id="p4contrast" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- GenStruct -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/genstruct.gif" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Compositionally Generalizable 3D Structure Prediction</a></b><br><br>Songfang Han, Jiayuan Gu, Kaichun Mo, <b>Li Yi</b>, Siyu Hu, Xuejin Chen and Hao Su<br><br>arXiv:2012.02493 [cs.CV], Dec 2020<br><br>
<a href="https://arxiv.org/abs/2012.02493">PDF</a>
</p><pre><p id="genstruct" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- NeuralRouting -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/neuralrouting.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments</a></b><br><br>Siyan Dong<sup>*</sup>, Qingnan Fan<sup>*</sup>, He Wang, Ji Shi, <b>Li Yi</b>, Thomas Funkhouser, Baoquan Chen, Leonidas J. Guibas (* equal contribution)<br><br>CVPR 2021 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/2012.04746">PDF</a>
</p><pre><p id="neuralrouting" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- CompLabel -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/complete_and_label.png" height="90" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Complete & Label: A Domain Adaptation Approach to Semantic Segmentation of LiDAR Point Clouds</a></b><br><br><b>Li Yi</b>, Boqing Gong, Thomas Funkhouser<br><br>CVPR 2021<br><br>
<a href="https://arxiv.org/abs/2007.08488">PDF</a>
</p><pre><p id="complabel" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- RethinkPCGan -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/rethinkpcgan.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Rethinking Sampling in 3D Point Cloud Generative Adversarial Networks</a></b><br><br>He Wang<sup>*</sup>, Zetian Jiang<sup>*</sup>, <b>Li Yi</b>, Kaichun Mo, Hao Su, Leonidas J. Guibas (* equal contribution)<br><br>CVPR 2021 Workshop on Learning to Generate 3D Shapes and Scenes<br><br>
<a href="https://arxiv.org/abs/2006.07029">PDF</a>
</p><pre><p id="rethinkpcgan" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- CSDF -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/csdf.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Curriculum DeepSDF</a></b><br><br>Yueqi Duan<sup>*</sup>, Haidong Zhu<sup>*</sup>, He Wang, <b>Li Yi</b>, Ram Nevatia, Leonidas J. Guibas (* equal contribution)<br><br>ECCV 2020<br><br>
<a href="https://arxiv.org/abs/2003.08593">PDF</a>
</p><pre><p id="csdf" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- Sapien -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/sapien.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">SAPIEN: A SimulAted Part-based Interactive ENvironment</a></b><br><br>Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, <b>Li Yi</b>, Angel X.Chang, Leonidas J. Guibas, Hao Su<br><br>CVPR 2020 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/2003.08515">PDF</a>
&nbsp<a href="http://sapien.ucsd.edu/publication">Project</a>
&nbsp<a href="https://github.com/haosulab/SAPIEN-Release">Code</a>
&nbsp<a href="https://youtu.be/K2yOeJhJXzM">Demo</a>
</p><pre><p id="sapien" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- ArticulatedPose -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/artpose.jpg" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Category-Level Articulated Object Pose Estimation</a></b><br><br>Xiaolong Li<sup>*</sup>, He Wang<sup>*</sup>, <b>Li Yi</b>, Leonidas Guibas, A. Lynn Abbott, Shuran Song (* equal contribution)<br><br>CVPR 2020 Oral Presentation<br><br>
<a href="https://articulated-pose.github.io/paper.pdf">PDF</a>
&nbsp<a href="https://articulated-pose.github.io/">Project</a>
</p><pre><p id="artpose" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- StructureEdit -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/structedit.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">StructEdit: Learning Structural Shape Variations</a></b><br><br>Kaichun Mo<sup>*</sup>, Paul Guerrero<sup>*</sup>, <b>Li Yi</b>, Hao Su, Peter Wonka, Niloy Mitra, Leonidas J. Guibas (* equal contribution)<br><br>CVPR 2020<br><br>
<a href="https://arxiv.org/abs/1911.11098">PDF</a>
&nbsp<a href="https://cs.stanford.edu/~kaichun/structedit/">Project</a>
</p><pre><p id="structedit" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- AdaCoSeg -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/zhu_adacoseg_long.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">AdaCoSeg: Adaptive Shape Co-Segmentation with Group Consistency Loss</a></b><br><br>Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, <b>Li Yi</b>, Leonidas J. Guibas, Hao Zhang<br><br>CVPR 2020 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/1903.10297">PDF</a>
</p><pre><p id="adaseg" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- StructureNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/structurenet.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">StructureNet: Hierarchical Graph Networks for 3D Shape Generation</a></b><br><br>Kaichun Mo<sup>*</sup>, Paul Guerrero<sup>*</sup>, <b>Li Yi</b>, Hao Su, Peter Wonka, Niloy Mitra, Leonidas J. Guibas (* equal contribution)<br><br>SIGGRAPH Asia 2019<br><br>
<a href="https://arxiv.org/abs/1908.00575">PDF</a>
&nbsp<a href="https://cs.stanford.edu/~kaichun/structurenet/">Project</a>
</p><pre><p id="structurenet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- GSPN -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/gspn.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud</a></b><br><br><b>Li Yi</b>, Wang Zhao, He Wang, Minhyuk Sung, Leonidas Guibas<br><br>CVPR 2019<br><br>
<a href="https://arxiv.org/abs/1812.03320">PDF</a>
&nbsp<a href="https://github.com/ericyi/GSPN">Code</a>
</p><pre><p id="gspn" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- TextureNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/texturenet.png" height="80" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">TextureNet: Consistent Local Parametrizations for Learning from High-Resolution Signals on Meshes</a></b><br><br>Jingwei Huang, Haotian Zhang, <b>Li Yi</b>, Thomas Funkhouser, Matthias Niessner, Leonidas Guibas<br><br>CVPR 2019 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/1812.00020">PDF</a>
</p><pre><p id="texturenet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PrimitiveFitNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/primitivefit.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Supervised Fitting of Geometric Primitives to 3D Point Clouds </a></b><br><br>Lingxiao Li<sup>*</sup>, Minhyuk Sung<sup>*</sup>, Anastasia Dubrovina, <b>Li Yi</b>, Leonidas Guibas (* equal contribution)<br><br>CVPR 2019 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/1811.08988">PDF</a>
&nbsp<a href="https://github.com/csimstu2/SPFN">Code</a>
</p><pre><p id="primitivefitnet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PartNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/partnet.png" height="140" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding</a></b><br><br>Kaichun Mo, Shilin Zhu, Angel X.Chang, <b>Li Yi</b>, Subarna Tripathi, Leonidas J. Guibas and Hao Su<br><br>CVPR 2019<br><br>
<a href="https://arxiv.org/abs/1812.02713">PDF</a>
&nbsp<a href="https://cs.stanford.edu/~kaichun/partnet/">Project</a>
</p><pre><p id="partnet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- GeoNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/geonet.jpg" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">GeoNet: Deep Geodesic Networks for Point Cloud Analysis</a></b><br><br>Tong He, Haibin Huang, <b>Li Yi</b>, Yuqian Zhou, Qihao Wu, Jue Wang, Stefano Soatto<br><br>CVPR 2019 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/1901.00680">PDF</a>
</p><pre><p id="geonet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PartMobility -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/partmobility.jpg" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Deep Part Induction from Articulated Object Pairs</a></b><br><br><b>Li Yi</b>, Haibin Huang, Difan Liu, Evangelos Kalogerakis, Hao Su, Leonidas Guibas<br><br>SIGGRAPH Asia 2018<br><br>
<a href="https://arxiv.org/pdf/1809.07417.pdf">PDF</a>
&nbsp<a href="https://github.com/ericyi/articulated-part-induction">Code</a>
</p><pre><p id="partmobility" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PartState -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/partstate.jpg" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">
Beyond Holistic Object Recognition: Enriching Image Understanding with Part States</a></b><br><br>Cewu Lu, Hao Su, Yongyi Lu, <b>Li Yi</b>, Chikeung Tang, Leonidas Guibas<br><br>CVPR 2018<br><br>
<a href="https://arxiv.org/pdf/1612.07310.pdf">PDF</a>
</p><pre><p id="partstate" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PointNet++ -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/pointnet2.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a></b><br><br>Charles R. Qi, <b>Li Yi</b>, Hao Su, Leonidas J. Guibas<br><br>NIPS 2017<br><br>
<a href="https://arxiv.org/pdf/1706.02413">PDF</a>
&nbsp<a href="http://stanford.edu/~rqi/pointnet2/">Project</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- HierSeg -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/hierseg.jpeg" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Learning Hierarchical Shape Segmentation and Labeling from Online Repositories</a></b><br><br><b>Li Yi</b>, Leonidas J. Guibas, Aaron Hertzmann, Vladimir G. Kim, Hao Su, Ersin Yumer <br><br>SIGGRAPH 2017<br><br>
<a href="https://arxiv.org/pdf/1705.01661.pdf">PDF</a>
&nbsp<a href="http://cs.stanford.edu/~ericyi/project_page/hier_seg/index.html">Project</a>
</p><pre><p id="hierseg" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- SyncSpecCNN -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/syncspeccnn.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation</a></b><br><br><b>Li Yi</b>, Hao Su, Xingwen Guo, Leonidas J. Guibas <br><br>CVPR 2017 Spotlight Presentation<br><br>
<a href="https://arxiv.org/pdf/1612.00606v1.pdf">PDF</a>
&nbsp<a href="https://github.com/ericyi/SyncSpecCNN">Code</a>
</p><pre><p id="syncspeccnn" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>




<!-- PartAnnotation -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/partanno.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">A Scalable Active Framework for Region Annotation in 3D Shape Collections</a></b><br><br><b>Li Yi</b>, Vladimir G. Kim, Duygu Ceylan, I-Chao Shen, Mengyuan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, Leonidas J. Guibas <br><br>SIGGRAPH Asia 2016<br><br>
<a href="http://cs.stanford.edu/~ericyi/papers/part_annotation_16.pdf">PDF(23.2MB)</a>
&nbsp<a href="http://cs.stanford.edu/~ericyi/papers/part_annotation_16_small.pdf">PDF(6.8MB)</a>
&nbsp<a href="http://cs.stanford.edu/~ericyi/papers/part_annotation_16_supplemental.pdf">Supplemental</a>
&nbsp<a href="http://cs.stanford.edu/~ericyi/project_page/part_annotation/index.html">Project</a>
&nbsp<a href="http://cs.stanford.edu/~ericyi/project_page/part_annotation/bib.html">Bib</a>
</p><pre><p id="partanno" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>



<!-- ShapeNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/shapenet.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="http://arxiv.org/abs/1512.03012">ShapeNet: An Information-Rich 3D Model Repository</a></b><br><br> Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, <b>Li Yi</b>, and Fisher Yu <br><br>arXiv:1512.03012 [cs.GR], Dec 2015<br><br>
<a href="http://arxiv.org/abs/1512.03012">PDF</a>
</p><pre><p id="shapenet" style="font:18px; display: none">
    @article{chang2015shapenet,
    title={Shapenet: An information-rich 3d model repository},
    author={Chang, Angel X and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and others},
    journal={arXiv preprint arXiv:1512.03012},
    year={2015}
    }
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- Novel View Feature Synthesis -->
<tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/surrogate.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="http://geometry.stanford.edu//papers/swyg-3afsnvo-15/swyg-3afsnvo-15.pdf">3D-Assisted Image Feature Synthesis for Novel Views of an Object</a></b><br><br>Hao Su<sup>*</sup>, Fan Wang<sup>*</sup>, <b>Li Yi</b>, Leonidas Guibas <br><br>ICCV 2015 oral<br><br>
<a href="http://geometry.stanford.edu//papers/swyg-3afsnvo-15/swyg-3afsnvo-15.pdf">PDF</a>
</p><pre><p id="surrogate" style="font:18px; display: none">
    @inproceedings{su20153d,
    title={3D-assisted feature synthesis for novel views of an object},
    author={Su, Hao and Wang, Fan and Yi, Eric and Guibas, Leonidas J},
    booktitle={Proceedings of the IEEE International Conference on Computer Vision},
    pages={2677--2685},
    year={2015}
    }
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- Super Resolution -->
<tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/superresolution.png" height="120" alt=""></p></td>
<td width="80%" valign="top">
<p>
<b><a href="http://cs.stanford.edu/~ericyi/papers/super_resolution_13.pdf">Image Super-Resolution Via Analysis Sparse Prior</a></b><br><br> Qiang Ning, Kan Chen, <b>Li Yi</b>, Chuchu Fan, Yao Lu, Jiangtao Wen <br><br>IEEE Signal Processing Letter 2013<br><br>
<a href="http://cs.stanford.edu/~ericyi/papers/super_resolution_13.pdf">PDF</a>
</p><pre><p id="superresolution" style="font:18px; display: none">
    @article{ning2013image,
    title={Image Super-Resolution Via Analysis Sparse Prior.},
    author={Ning, Qiang and Chen, Kan and Yi, Li and Fan, Chuchu and Lu, Yao and Wen, Jiangtao},
    journal={IEEE Signal Process. Lett.},
    volume={20},
    number={4},
    pages={399--402},
    year={2013}
    }
</p><p></p></pre>
<p></p></td>
</tr>

</tbody></table>

</ul>
</div>
<br>

<hr>
<div id="footer" style="font-size:10">Li Yi <i>Last updated: Mar, 2023</i></div>

</body></html>
