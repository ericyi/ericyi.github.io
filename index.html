
<!-- saved from url=(0029)http://web.stanford.edu/~ericyi/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=GBK">
<title>Li Yi</title>
<link rel="stylesheet" type="text/css" href="./Li_Yi_files/main.css">
</head>

<body>

<table>
<tbody><tr>
<td><img src="./Li_Yi_files/ericyi.jpg" width="200"></td>
<td>
<div style="font-size:24; font-weight:bold">Li Yi (弋力)</div>
<div>
Tenure-track Assistant Professor at Tsinghua University<br>
助理教授 博士生导师 清华大学交叉信息研究院（IIIS）<br>
</div>
<div>
<b>Email:</b> <tt>ericyi0124 at gmail dot com</tt><br>
</div>
<div>
<a href="https://scholar.google.com/citations?user=UyZL660AAAAJ&hl=en">[Google Scholar]</a><a href="https://twitter.com/ericyi0124">[Twitter]</a>
</div>
</td>
</tr>
</tbody></table>

<script type="text/javascript">
function hideshow(which){
if (!document.getElementById)
return
if (which.style.display=="block")
which.style.display="none"
else
which.style.display="block"
}
</script>



<h3>About</h3>
<div class="section">
<ul>
I am an Assistant Professor in the <a href="https://iiis.tsinghua.edu.cn/">Institute for Interdisciplinary Information Sciences (IIIS)</a> at Tsinghua University. I received my Ph.D. from Stanford University, advised by Professor <a href="http://geometry.stanford.edu/member/guibas/index.html">Leonidas J. Guibas</a>. And I spent a wonderful time at Google as a Research Scientist after graduation, working closely with Professor <a href="https://www.cs.princeton.edu/~funk/">Thomas Funkhouser</a>. Prior to joining Stanford, I got my bachelor degree in Electronic Engineering from <a href="http://www.tsinghua.edu.cn/">Tsinghua University</a>.</ul>
<ul>
My recent research interests focus on 3D perception and shape analysis, with the goal of equipping robotic agent with the ability of understanding and interacting with the 3D world. I am actively looking for motivated PostDoc/PhD/BS students in my lab. Please feel free to email me with your resume if you are interested.
</ul>
</div>
<br>

<div class="section">
<h3>News</h3>
<ul>
<li> <b style="color: green; background-color: #ffff42">NEW</b> [2022/03] Seven papers get accepted to CVPR 2022.
<li> [2021/09] Two papers get accepted to NeurIPS 2021 and one paper get accepted to ICCV 2021.
<li> [2021/05] I will serve as an Area Chair for <a href="http://cvpr2022.thecvf.com/">CVPR 2022</a>.
<li> [2021/05] I am organizing <a href="https://iccv21-seai.github.io/">The 1st Workshop on Simulation Technology for Embodied AI</a> at <a href="http://iccv2021.thecvf.com/home">ICCV 2021</a>.
<li> [2021/03] Two papers accepted at CVPR 2021 (one oral included).
</ul>
</div>
<br>

<a name="projects"></a>
<h3>Recent Projects</h3>
<div class="mainsection">
<ul>

*: equivalent contribution, <span>†</span>: corresponding author
<br><br>


<table width="100%">


<!-- Thesis -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/thesis.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Deep Object-Centric 3D Perception</a></b><br><br><b>Li Yi</b><br><br>Ph.D. Thesis<br><br>
<a href="https://searchworks.stanford.edu/view/13333376">PDF</a>
</p><pre><p id="thesis" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- HOI4D -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/hoi4d.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object Interaction</a></b><br><br>Yunze Liu*, Yun Liu*, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, <b>Li Yi<span>†</span></b><br><br>CVPR 2022<br><br>
<a href="https://arxiv.org/abs/2203.01577">PDF</a>
&nbsp<a href="https://hoi4d.github.io">Project</a>
</p><pre><p id="hoi4d" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- EON -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/eon.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Rotationally Equivariant 3D Object Detection</a></b><br><br>Hong-Xing Yu, Jiajun Wu, <b>Li Yi<span>†</span></b><br><br>CVPR 2022<br><br>
<a href="https://kovenyu.com/eon/static/yu2022eon.pdf">PDF</a>
&nbsp<a href="https://kovenyu.com/eon/">Project</a>
</p><pre><p id="eon" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- AutoGPart -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/autogpart.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">AutoGPart: Intermediate Supervision Search for Generalizable 3D Part Segmentation</a></b><br><br>Xueyi Liu, Xiaomeng Xu, Anyi Rao, Chuang Gan, <b>Li Yi<span>†</span></b><br><br>CVPR 2022<br><br>
<a href="https://arxiv.org/abs/2203.06558">PDF</a>
&nbsp<a href="https://autogpart.github.io">Project</a>
</p><pre><p id="autogpart" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- CodedVTR -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/codedvtr.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">CodedVTR: Codebook-based Sparse Voxel Transformer with Geometric Guidance</a></b><br><br>Tianchen Zhao, Niansong Zhang, Xuefei Ning, He Wang, <b>Li Yi<span>†</span></b>, Yu Wang<br><br>CVPR 2022<br><br>
<a href="https://arxiv.org/abs/2203.09887">PDF</a>
&nbsp<a href="https://a-suozhang.xyz/codedvtr.github.io">Project</a>
</p><pre><p id="codedvtr" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- MultiRobotMap -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/multirobotmap.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Multi-Robot Active Mapping via Neural Bipartite Graph Matching</a></b><br><br>Kai Ye*, Siyan Dong*, Qingnan Fan, He Wang, <b>Li Yi</b>, Fei Xia, Jue Wang, Baoquan Chen<br><br>CVPR 2022<br><br>
<a href="https://arxiv.org/abs/2203.16319">PDF</a>
</p><pre><p id="multirobotmap" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- APES -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/apes.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">APES: Articulated Part Extraction from Sprite Sheets</a></b><br><br>Zhan Xu, Matthew Fisher, Yang Zhou, Deepali Aneja, Rushikesh Dudhat, <b>Li Yi</b>, Evangelos Kalogerakis<br><br>CVPR 2022<br><br>
<a href="">PDF</a>
&nbsp<a href="https://zhan-xu.github.io/parts">Project</a>
</p><pre><p id="apes" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- FixingMal -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/fixingmal.png" width="250" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Fixing Malfunctional Objects With Learned Physical Simulation and Functional Prediction</a></b><br><br>Yining Hong, Kaichun Mo, <b>Li Yi</b>, Leonidas J. Guibas, Antonio Torralba, Joshua B. Tenenbaum, Chuang Gan<br><br>CVPR 2022<br><br>
<a href="http://fixing-malfunctional.csail.mit.edu/assets/00889.pdf">PDF</a>
&nbsp<a href="http://fixing-malfunctional.csail.mit.edu">Project</a>
</p><pre><p id="fixingmal" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PTR -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/ptr.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">PTR: A Benchmark for Part-based Conceptual, Relational, and Physical Reasoning</a></b><br><br>Yining Hong, <b>Li Yi</b>, Joshua B. Tenenbaum, Antonio Torralba, Chuang Gan<br><br>NeurIPS 2021<br><br>
<a href="https://arxiv.org/abs/2112.05136">PDF</a>
</p><pre><p id="ptr" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- ESSCOP -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/esscop.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Leveraging SE(3) Equivariance for Self-supervised Category-Level Object Pose Estimation from Point Clouds</a></b><br><br>Xiaolong Li, Yijia Weng, <b>Li Yi</b>, Leonidas J. Guibas, A. Lynn Abbott, Shuran Song, He Wang<br><br>NeurIPS 2021<br><br>
<a href="https://arxiv.org/abs/2111.00190">PDF</a>
</p><pre><p id="esscop" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- TupleInfoNCE -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/tupleinfonce.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Contrastive Multimodal Fusion with TupleInfoNCE</a></b><br><br>Yunze Liu, Qingnan Fan, Shanghang Zhang, Hao Dong, Thomas Funkhouser, <b>Li Yi<span>†</span></b><br><br>ICCV 2021<br><br>
<a href="https://arxiv.org/abs/2107.02575">PDF</a>
</p><pre><p id="tupleinfonce" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- P4Contrast -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/p4contrast.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">P4Contrast: Contrastive Learning with Pairs of Point-Pixel Pairs for RGB-D Scene Understanding</a></b><br><br>Yunze Liu<sup>*</sup>, <b>Li Yi</b><sup>*</sup>, Shanghang Zhang, Qingnan Fan, Thomas Funkhouser, Hao Dong (* equal contribution)<br><br>arXiv:2012.13089 [cs.CV], Dec 2020<br><br>
<a href="https://arxiv.org/abs/2012.13089">PDF</a>
</p><pre><p id="p4contrast" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- GenStruct -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/genstruct.gif" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Compositionally Generalizable 3D Structure Prediction</a></b><br><br>Songfang Han, Jiayuan Gu, Kaichun Mo, <b>Li Yi</b>, Siyu Hu, Xuejin Chen and Hao Su<br><br>arXiv:2012.02493 [cs.CV], Dec 2020<br><br>
<a href="https://arxiv.org/abs/2012.02493">PDF</a>
</p><pre><p id="genstruct" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- NeuralRouting -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/neuralrouting.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments</a></b><br><br>Siyan Dong<sup>*</sup>, Qingnan Fan<sup>*</sup>, He Wang, Ji Shi, <b>Li Yi</b>, Thomas Funkhouser, Baoquan Chen, Leonidas J. Guibas (* equal contribution)<br><br>CVPR 2021 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/2012.04746">PDF</a>
</p><pre><p id="neuralrouting" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- CompLabel -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/complete_and_label.png" height="90" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Complete & Label: A Domain Adaptation Approach to Semantic Segmentation of LiDAR Point Clouds</a></b><br><br><b>Li Yi</b>, Boqing Gong, Thomas Funkhouser<br><br>CVPR 2021<br><br>
<a href="https://arxiv.org/abs/2007.08488">PDF</a>
</p><pre><p id="complabel" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- RethinkPCGan -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/rethinkpcgan.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Rethinking Sampling in 3D Point Cloud Generative Adversarial Networks</a></b><br><br>He Wang<sup>*</sup>, Zetian Jiang<sup>*</sup>, <b>Li Yi</b>, Kaichun Mo, Hao Su, Leonidas J. Guibas (* equal contribution)<br><br>arXiv:2006.07029 [cs.CV], June 2020<br><br>
<a href="https://arxiv.org/abs/2006.07029">PDF</a>
</p><pre><p id="rethinkpcgan" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- CSDF -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/csdf.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Curriculum DeepSDF</a></b><br><br>Yueqi Duan<sup>*</sup>, Haidong Zhu<sup>*</sup>, He Wang, <b>Li Yi</b>, Ram Nevatia, Leonidas J. Guibas (* equal contribution)<br><br>ECCV 2020<br><br>
<a href="https://arxiv.org/abs/2003.08593">PDF</a>
</p><pre><p id="csdf" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- Sapien -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/sapien.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">SAPIEN: A SimulAted Part-based Interactive ENvironment</a></b><br><br>Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu Yuan, He Wang, <b>Li Yi</b>, Angel X.Chang, Leonidas J. Guibas, Hao Su<br><br>CVPR 2020 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/2003.08515">PDF</a>
&nbsp<a href="http://sapien.ucsd.edu/publication">Project</a>
&nbsp<a href="https://github.com/haosulab/SAPIEN-Release">Code</a>
&nbsp<a href="https://youtu.be/K2yOeJhJXzM">Demo</a>
</p><pre><p id="sapien" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- ArticulatedPose -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/artpose.jpg" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Category-Level Articulated Object Pose Estimation</a></b><br><br>Xiaolong Li<sup>*</sup>, He Wang<sup>*</sup>, <b>Li Yi</b>, Leonidas Guibas, A. Lynn Abbott, Shuran Song (* equal contribution)<br><br>CVPR 2020 Oral Presentation<br><br>
<a href="https://articulated-pose.github.io/paper.pdf">PDF</a>
&nbsp<a href="https://articulated-pose.github.io/">Project</a>
</p><pre><p id="artpose" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- StructureEdit -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/structedit.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">StructEdit: Learning Structural Shape Variations</a></b><br><br>Kaichun Mo<sup>*</sup>, Paul Guerrero<sup>*</sup>, <b>Li Yi</b>, Hao Su, Peter Wonka, Niloy Mitra, Leonidas J. Guibas (* equal contribution)<br><br>CVPR 2020<br><br>
<a href="https://arxiv.org/abs/1911.11098">PDF</a>
&nbsp<a href="https://cs.stanford.edu/~kaichun/structedit/">Project</a>
</p><pre><p id="structedit" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- AdaCoSeg -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/zhu_adacoseg_long.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">AdaCoSeg: Adaptive Shape Co-Segmentation with Group Consistency Loss</a></b><br><br>Chenyang Zhu, Kai Xu, Siddhartha Chaudhuri, <b>Li Yi</b>, Leonidas J. Guibas, Hao Zhang<br><br>CVPR 2020 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/1903.10297">PDF</a>
</p><pre><p id="adaseg" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- StructureNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/structurenet.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">StructureNet: Hierarchical Graph Networks for 3D Shape Generation</a></b><br><br>Kaichun Mo<sup>*</sup>, Paul Guerrero<sup>*</sup>, <b>Li Yi</b>, Hao Su, Peter Wonka, Niloy Mitra, Leonidas J. Guibas (* equal contribution)<br><br>SIGGRAPH Asia 2019<br><br>
<a href="https://arxiv.org/abs/1908.00575">PDF</a>
&nbsp<a href="https://cs.stanford.edu/~kaichun/structurenet/">Project</a>
</p><pre><p id="structurenet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- GSPN -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/gspn.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">GSPN: Generative Shape Proposal Network for 3D Instance Segmentation in Point Cloud</a></b><br><br><b>Li Yi</b>, Wang Zhao, He Wang, Minhyuk Sung, Leonidas Guibas<br><br>CVPR 2019<br><br>
<a href="https://arxiv.org/abs/1812.03320">PDF</a>
&nbsp<a href="https://github.com/ericyi/GSPN">Code</a>
</p><pre><p id="gspn" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- TextureNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/texturenet.png" height="80" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">TextureNet: Consistent Local Parametrizations for Learning from High-Resolution Signals on Meshes</a></b><br><br>Jingwei Huang, Haotian Zhang, <b>Li Yi</b>, Thomas Funkhouser, Matthias Niessner, Leonidas Guibas<br><br>CVPR 2019 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/1812.00020">PDF</a>
</p><pre><p id="texturenet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PrimitiveFitNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/primitivefit.png" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Supervised Fitting of Geometric Primitives to 3D Point Clouds </a></b><br><br>Lingxiao Li<sup>*</sup>, Minhyuk Sung<sup>*</sup>, Anastasia Dubrovina, <b>Li Yi</b>, Leonidas Guibas (* equal contribution)<br><br>CVPR 2019 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/1811.08988">PDF</a>
&nbsp<a href="https://github.com/csimstu2/SPFN">Code</a>
</p><pre><p id="primitivefitnet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PartNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/partnet.png" height="140" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">PartNet: A Large-scale Benchmark for Fine-grained and Hierarchical Part-level 3D Object Understanding</a></b><br><br>Kaichun Mo, Shilin Zhu, Angel X.Chang, <b>Li Yi</b>, Subarna Tripathi, Leonidas J. Guibas and Hao Su<br><br>CVPR 2019<br><br>
<a href="https://arxiv.org/abs/1812.02713">PDF</a>
&nbsp<a href="https://cs.stanford.edu/~kaichun/partnet/">Project</a>
</p><pre><p id="partnet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- GeoNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/geonet.jpg" height="100" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">GeoNet: Deep Geodesic Networks for Point Cloud Analysis</a></b><br><br>Tong He, Haibin Huang, <b>Li Yi</b>, Yuqian Zhou, Qihao Wu, Jue Wang, Stefano Soatto<br><br>CVPR 2019 Oral Presentation<br><br>
<a href="https://arxiv.org/abs/1901.00680">PDF</a>
</p><pre><p id="geonet" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PartMobility -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/partmobility.jpg" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Deep Part Induction from Articulated Object Pairs</a></b><br><br><b>Li Yi</b>, Haibin Huang, Difan Liu, Evangelos Kalogerakis, Hao Su, Leonidas Guibas<br><br>SIGGRAPH Asia 2018<br><br>
<a href="https://arxiv.org/pdf/1809.07417.pdf">PDF</a>
&nbsp<a href="https://github.com/ericyi/articulated-part-induction">Code</a>
</p><pre><p id="partmobility" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PartState -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/partstate.jpg" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">
Beyond Holistic Object Recognition: Enriching Image Understanding with Part States</a></b><br><br>Cewu Lu, Hao Su, Yongyi Lu, <b>Li Yi</b>, Chikeung Tang, Leonidas Guibas<br><br>CVPR 2018<br><br>
<a href="https://arxiv.org/pdf/1612.07310.pdf">PDF</a>
</p><pre><p id="partstate" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- PointNet++ -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/pointnet2.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</a></b><br><br>Charles R. Qi, <b>Li Yi</b>, Hao Su, Leonidas J. Guibas<br><br>NIPS 2017<br><br>
<a href="https://arxiv.org/pdf/1706.02413">PDF</a>
&nbsp<a href="http://stanford.edu/~rqi/pointnet2/">Project</a>
</p><pre><p id="pointnet++" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- HierSeg -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/hierseg.jpeg" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">Learning Hierarchical Shape Segmentation and Labeling from Online Repositories</a></b><br><br><b>Li Yi</b>, Leonidas J. Guibas, Aaron Hertzmann, Vladimir G. Kim, Hao Su, Ersin Yumer <br><br>SIGGRAPH 2017<br><br>
<a href="https://arxiv.org/pdf/1705.01661.pdf">PDF</a>
&nbsp<a href="http://cs.stanford.edu/~ericyi/project_page/hier_seg/index.html">Project</a>
</p><pre><p id="hierseg" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- SyncSpecCNN -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/syncspeccnn.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">SyncSpecCNN: Synchronized Spectral CNN for 3D Shape Segmentation</a></b><br><br><b>Li Yi</b>, Hao Su, Xingwen Guo, Leonidas J. Guibas <br><br>CVPR 2017 Spotlight Presentation<br><br>
<a href="https://arxiv.org/pdf/1612.00606v1.pdf">PDF</a>
&nbsp<a href="https://github.com/ericyi/SyncSpecCNN">Code</a>
</p><pre><p id="syncspeccnn" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>




<!-- PartAnnotation -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/partanno.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="">A Scalable Active Framework for Region Annotation in 3D Shape Collections</a></b><br><br><b>Li Yi</b>, Vladimir G. Kim, Duygu Ceylan, I-Chao Shen, Mengyuan Yan, Hao Su, Cewu Lu, Qixing Huang, Alla Sheffer, Leonidas J. Guibas <br><br>SIGGRAPH Asia 2016<br><br>
<a href="http://cs.stanford.edu/~ericyi/papers/part_annotation_16.pdf">PDF(23.2MB)</a>
&nbsp<a href="http://cs.stanford.edu/~ericyi/papers/part_annotation_16_small.pdf">PDF(6.8MB)</a>
&nbsp<a href="http://cs.stanford.edu/~ericyi/papers/part_annotation_16_supplemental.pdf">Supplemental</a>
&nbsp<a href="http://cs.stanford.edu/~ericyi/project_page/part_annotation/index.html">Project</a>
&nbsp<a href="http://cs.stanford.edu/~ericyi/project_page/part_annotation/bib.html">Bib</a>
</p><pre><p id="partanno" style="font:18px; display: none">
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>



<!-- ShapeNet -->
<tbody><tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/shapenet.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="http://arxiv.org/abs/1512.03012">ShapeNet: An Information-Rich 3D Model Repository</a></b><br><br> Angel X. Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, <b>Li Yi</b>, and Fisher Yu <br><br>arXiv:1512.03012 [cs.GR], Dec 2015<br><br>
<a href="http://arxiv.org/abs/1512.03012">PDF</a>
</p><pre><p id="shapenet" style="font:18px; display: none">
    @article{chang2015shapenet,
    title={Shapenet: An information-rich 3d model repository},
    author={Chang, Angel X and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and others},
    journal={arXiv preprint arXiv:1512.03012},
    year={2015}
    }
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- Novel View Feature Synthesis -->
<tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/surrogate.png" height="120" alt="" style="border-style: none" align="top"></p></td>
<td width="80%" valign="top"><p>
    <b><a href="http://geometry.stanford.edu//papers/swyg-3afsnvo-15/swyg-3afsnvo-15.pdf">3D-Assisted Image Feature Synthesis for Novel Views of an Object</a></b><br><br>Hao Su<sup>*</sup>, Fan Wang<sup>*</sup>, <b>Li Yi</b>, Leonidas Guibas <br><br>ICCV 2015 oral<br><br>
<a href="http://geometry.stanford.edu//papers/swyg-3afsnvo-15/swyg-3afsnvo-15.pdf">PDF</a>
</p><pre><p id="surrogate" style="font:18px; display: none">
    @inproceedings{su20153d,
    title={3D-assisted feature synthesis for novel views of an object},
    author={Su, Hao and Wang, Fan and Yi, Eric and Guibas, Leonidas J},
    booktitle={Proceedings of the IEEE International Conference on Computer Vision},
    pages={2677--2685},
    year={2015}
    }
</p><p></p></pre>
<p></p></td>
</tr>

<tr><td><br></td></tr>

<!-- Super Resolution -->
<tr>
<td width="20%" valign="top"><p><img src="./Li_Yi_files/superresolution.png" height="120" alt=""></p></td>
<td width="80%" valign="top">
<p>
<b><a href="http://cs.stanford.edu/~ericyi/papers/super_resolution_13.pdf">Image Super-Resolution Via Analysis Sparse Prior</a></b><br><br> Qiang Ning, Kan Chen, <b>Li Yi</b>, Chuchu Fan, Yao Lu, Jiangtao Wen <br><br>IEEE Signal Processing Letter 2013<br><br>
<a href="http://cs.stanford.edu/~ericyi/papers/super_resolution_13.pdf">PDF</a>
</p><pre><p id="superresolution" style="font:18px; display: none">
    @article{ning2013image,
    title={Image Super-Resolution Via Analysis Sparse Prior.},
    author={Ning, Qiang and Chen, Kan and Yi, Li and Fan, Chuchu and Lu, Yao and Wen, Jiangtao},
    journal={IEEE Signal Process. Lett.},
    volume={20},
    number={4},
    pages={399--402},
    year={2013}
    }
</p><p></p></pre>
<p></p></td>
</tr>

</tbody></table>

</ul>
</div>
<br>

<hr>
<div id="footer" style="font-size:10">Li Yi <i>Last updated: Mar, 2022</i></div>

</body></html>
